"""
app.py - Ù†Ø³Ø®Ù‡ Ø¢Ù¾Ø¯ÛŒØª Ø´Ø¯Ù‡ Ø¨Ø§ SeleniumBase
"""

import pandas as pd
import gspread
from oauth2client.service_account import ServiceAccountCredentials
import tldextract
import time
import json

from google_module import GoogleSerper
from hybrid_backlink_v2 import HybridBacklinkAnalyzerV2


# =====================================
# Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ú¯ÙˆÚ¯Ù„ Ø´ÛŒØª
# =====================================
def connect_to_sheet(sheet_name):
    scope = [
        "https://spreadsheets.google.com/feeds",
        "https://www.googleapis.com/auth/drive"
    ]
    creds = ServiceAccountCredentials.from_json_keyfile_name(
        "credentials.json", scope
    )
    client = gspread.authorize(creds)
    return client.open(sheet_name).sheet1


# =====================================
# Flatten Ú©Ø§Ù…Ù„ Ø®Ø±ÙˆØ¬ÛŒ API (ØªÙÚ©ÛŒÚ© Ø³ØªÙˆÙ†â€ŒÙ‡Ø§)
# =====================================
def flatten_link(link: dict):
    flat = {}
    for k, v in link.items():
        if isinstance(v, (dict, list)):
            flat[k] = json.dumps(v, ensure_ascii=False)
        else:
            flat[k] = v
    return flat


# =====================================
# Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø§ØµÙ„ÛŒ
# =====================================
def main():
    print("="*70)
    print("ğŸ”¥ SEO BACKLINK ANALYZER - V2")
    print("="*70)
    print("\nÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯:")
    print("  âœ… Ahrefs scraping Ø¨Ø§ SeleniumBase (Ø±Ø§ÛŒÚ¯Ø§Ù†ØŒ 80-90% Ù…ÙˆÙÙ‚ÛŒØª)")
    print("  âœ… SE Ranking API (Ù¾ÙˆÙ„ÛŒØŒ 100% Ù…ÙˆÙÙ‚ÛŒØª)")
    print("  âœ… Fallback Ù‡ÙˆØ´Ù…Ù†Ø¯")
    print("  âœ… Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Google Sheets")
    print("="*70 + "\n")
    
    # Ø¯Ø±ÛŒØ§ÙØª ÙˆØ±ÙˆØ¯ÛŒâ€ŒÙ‡Ø§
    keyword = input("ğŸ”‘ Ú©Ù„Ù…Ù‡ Ú©Ù„ÛŒØ¯ÛŒ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯: ")
    sheet_name = input("ğŸ“Š Ù†Ø§Ù… Google Sheet [SEO_Report]: ").strip() or "SEO_Report"
    
    # Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ù†Ø¨Ø¹ Ø¨Ú©â€ŒÙ„ÛŒÙ†Ú©
    print("\nğŸ”— Ù…Ù†Ø¨Ø¹ Ø¨Ú©â€ŒÙ„ÛŒÙ†Ú©:")
    print("  1. Ahrefs (SeleniumBase) - Ø±Ø§ÛŒÚ¯Ø§Ù†")
    print("  2. SE Ranking - API")
    print("  3. Ù‡Ø± Ø¯Ùˆ (Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ)")
    
    choice = input("\nØ§Ù†ØªØ®Ø§Ø¨ [3]: ").strip() or "3"
    
    use_ahrefs = choice in ["1", "3"]
    use_se_ranking = choice in ["2", "3"]
    prefer_ahrefs = choice == "1"
    
    # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ahrefs
    headless = False
    if use_ahrefs:
        headless_input = input("ğŸ–¥ï¸  Ahrefs Ø¯Ø± Ø­Ø§Ù„Øª HeadlessØŸ (y/n) [n]: ").strip().lower()
        headless = headless_input == 'y'
    
    # Ø§Ø¨Ø²Ø§Ø±Ù‡Ø§
    google_tool = GoogleSerper()
    
    backlink_tool = HybridBacklinkAnalyzerV2(
        prefer_ahrefs=prefer_ahrefs,
        use_seleniumbase=use_ahrefs,
        headless=headless
    )
    
    print(f"\n{'='*70}")
    print(f"ğŸš€ Ø´Ø±ÙˆØ¹ Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ú¯ÙˆÚ¯Ù„ Ø¨Ø±Ø§ÛŒ: {keyword}")
    print("="*70 + "\n")
    
    serp_results = google_tool.get_competitors(keyword)
    
    if not serp_results:
        print("âŒ Ù‡ÛŒÚ† Ù†ØªÛŒØ¬Ù‡â€ŒØ§ÛŒ Ø§Ø² Ú¯ÙˆÚ¯Ù„ Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯.")
        return
    
    final_rows = []
    all_headers = set()
    
    print(f"âœ… {len(serp_results)} Ø³Ø§ÛŒØª Ø±Ù‚ÛŒØ¨ Ù¾ÛŒØ¯Ø§ Ø´Ø¯.\n")
    
    # Ù†Ù…Ø§ÛŒØ´ Ø±Ù‚Ø¨Ø§
    print("ğŸ“‹ Ù„ÛŒØ³Øª Ø±Ù‚Ø¨Ø§:")
    for i, item in enumerate(serp_results, 1):
        domain = tldextract.extract(item.get("link", "")).registered_domain
        print(f"  {i}. {domain} (Ø±ØªØ¨Ù‡ {item.get('position', 'N/A')})")
    
    print(f"\n{'='*70}")
    print("ğŸ”— Ø´Ø±ÙˆØ¹ ØªØ­Ù„ÛŒÙ„ Ø¨Ú©â€ŒÙ„ÛŒÙ†Ú©â€ŒÙ‡Ø§...")
    print("="*70 + "\n")
    
    for idx, item in enumerate(serp_results, 1):
        url = item.get("link")
        if not url:
            continue
        
        domain = tldextract.extract(url).registered_domain
        position = item.get("position")
        
        print(f"\n[{idx}/{len(serp_results)}] ğŸ“ {domain}")
        print("-" * 60)
        
        # Ø¯Ø±ÛŒØ§ÙØª Ø¨Ú©â€ŒÙ„ÛŒÙ†Ú©â€ŒÙ‡Ø§
        links = backlink_tool.fetch_links(
            domain,
            limit=20,  # Ù…ÛŒâ€ŒØªÙˆÙ†ÛŒ ØªØºÛŒÛŒØ± Ø¨Ø¯ÛŒ
            fallback=True,
            ahrefs_wait_time=10
        )
        
        if not links:
            print(f"âš ï¸ Ù‡ÛŒÚ† Ø¨Ú©â€ŒÙ„ÛŒÙ†Ú©ÛŒ Ø¨Ø±Ø§ÛŒ {domain} Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯")
            row = {
                "Keyword": keyword,
                "Google Pos": position,
                "Competitor": domain,
                "status": "No backlinks found"
            }
            final_rows.append(row)
            all_headers.update(row.keys())
            continue
        
        print(f"âœ… {len(links)} Ø¨Ú©â€ŒÙ„ÛŒÙ†Ú© Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯\n")
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¨Ù‡ Ù†ØªØ§ÛŒØ¬
        for link in links:
            flat_link = flatten_link(link)
            
            row = {
                "Keyword": keyword,
                "Google Pos": position,
                "Competitor": domain,
                **flat_link
            }
            
            final_rows.append(row)
            all_headers.update(row.keys())
        
        # ØªØ§Ø®ÛŒØ± Ø¨ÛŒÙ† Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§
        if idx < len(serp_results):
            wait = 5
            print(f"â³ ØµØ¨Ø± {wait} Ø«Ø§Ù†ÛŒÙ‡ ØªØ§ Ø±Ù‚ÛŒØ¨ Ø¨Ø¹Ø¯ÛŒ...\n")
            time.sleep(wait)
    
    if not final_rows:
        print("\nâŒ Ø¯ÛŒØªØ§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")
        return
    
    headers = list(all_headers)
    
    # =====================================
    # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ú¯ÙˆÚ¯Ù„ Ø´ÛŒØª
    # =====================================
    print(f"\n{'='*70}")
    print("ğŸ’¾ Ø¯Ø± Ø­Ø§Ù„ Ø§Ø±Ø³Ø§Ù„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ Ú¯ÙˆÚ¯Ù„ Ø´ÛŒØª...")
    print("="*70)
    
    try:
        sheet = connect_to_sheet(sheet_name)
        existing_data = sheet.get_all_values()
        
        # Ø§Ú¯Ø± Ø´ÛŒØª Ø®Ø§Ù„ÛŒ Ø§Ø³ØªØŒ Ù‡Ø¯Ø±Ù‡Ø§ Ø±Ø§ Ø¨Ø³Ø§Ø²
        if not existing_data:
            sheet.append_row(headers)
            print("âœ… Ù‡Ø¯Ø±Ù‡Ø§ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯")
        
        rows_to_append = [
            [row.get(h, "N/A") for h in headers]
            for row in final_rows
        ]
        
        sheet.append_rows(rows_to_append)
        print(f"âœ… {len(rows_to_append)} Ø±Ø¯ÛŒÙ Ø¨Ù‡ Ø´ÛŒØª Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯")
        print("ğŸ¯ Ø´ÛŒØª Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¢Ù¾Ø¯ÛŒØª Ø´Ø¯!")
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ÛŒ Ú¯ÙˆÚ¯Ù„ Ø´ÛŒØª: {e}")
        print("\nğŸ’¾ Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„ CSV Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† backup...")
        
        df = pd.DataFrame(final_rows)
        filename = f"backup_results_{keyword.replace(' ', '_')}.csv"
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        print(f"âœ… ÙØ§ÛŒÙ„ {filename} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")
    
    # Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬
    print(f"\n{'='*70}")
    print("ğŸ“Š Ø®Ù„Ø§ØµÙ‡ Ù†ØªØ§ÛŒØ¬:")
    print("="*70)
    print(f"  ğŸ”‘ Ú©Ù„Ù…Ù‡ Ú©Ù„ÛŒØ¯ÛŒ: {keyword}")
    print(f"  ğŸ† ØªØ¹Ø¯Ø§Ø¯ Ø±Ù‚Ø¨Ø§: {len(serp_results)}")
    print(f"  ğŸ”— Ú©Ù„ Ø¨Ú©â€ŒÙ„ÛŒÙ†Ú©â€ŒÙ‡Ø§: {len(final_rows)}")
    print(f"  ğŸ“Š Google Sheet: {sheet_name}")
    print("="*70 + "\n")


if __name__ == "__main__":
    main()
