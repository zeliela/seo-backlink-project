
"""
Ahrefs Scraper Ø¨Ø§ SeleniumBase UC Mode
# Ø¨Ù‡ØªØ±ÛŒÙ† Ø±ÙˆØ´ Ø¨Ø±Ø§ÛŒ bypass Ú©Ø±Ø¯Ù† Cloudflare!
"""

from seleniumbase import SB
import time
import json
from typing import List, Dict, Optional


class AhrefsSeleniumBase:
    def __init__(self, headless: bool = False):
        """
        Args:
            headless: False Ø¨Ø±Ø§ÛŒ Ø¯ÛŒØ¯Ù† Ù…Ø±ÙˆØ±Ú¯Ø± (Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ)
        """
        self.headless = headless
        self.base_url = "https://ahrefs.com/backlink-checker"
    
    def fetch_backlinks(self, domain: str, wait_time: int = 10) -> List[Dict]:
        """
#         Ø¯Ø±ÛŒØ§ÙØª Ø¨Ú©â€ŒÙ„ÛŒÙ†Ú© Ø¨Ø§ SeleniumBase UC mode
        
        Args:
            domain: Ø¯Ø§Ù…Ù†Ù‡ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø±
            wait_time: Ø²Ù…Ø§Ù† Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ø±Ø§ÛŒ Ù„ÙˆØ¯ Ø´Ø¯Ù† Ø¬Ø¯ÙˆÙ„ (Ø«Ø§Ù†ÛŒÙ‡)
        
        Returns:
#             Ù„ÛŒØ³Øª Ø¨Ú©â€ŒÙ„ÛŒÙ†Ú©â€ŒÙ‡Ø§
        """
        backlinks = []
        
        print(f"ðŸš€ Ø´Ø±ÙˆØ¹ scraping Ø¨Ø±Ø§ÛŒ: {domain}")
        
        with SB(uc=True, test=True, headless=self.headless, locale_code="en") as sb:
            try:
#                 # Ø¨Ø§Ø² Ú©Ø±Ø¯Ù† ØµÙØ­Ù‡ Ø¨Ø§ reconnect (Ø¨Ø±Ø§ÛŒ bypass Cloudflare)
                print("ðŸŒ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø² Ú©Ø±Ø¯Ù† Ahrefs...")
                url = f"{self.base_url}?input={domain}&mode=subdomains"
                sb.driver.uc_open_with_reconnect(url, reconnect_time=4)
                
                print("â³ ØµØ¨Ø± Ø¨Ø±Ø§ÛŒ bypass Cloudflare...")
                time.sleep(5)
                
#                 # Ú†Ú© Ú©Ø±Ø¯Ù† Ø¹Ù†ÙˆØ§Ù† ØµÙØ­Ù‡
                print(f"ðŸ“„ Ø¹Ù†ÙˆØ§Ù† ØµÙØ­Ù‡: {sb.get_title()}")
                
#                 # ØµØ¨Ø± Ø¨Ø±Ø§ÛŒ Ù„ÙˆØ¯ Ø´Ø¯Ù† Ø¬Ø¯ÙˆÙ„
                print(f"â³ ØµØ¨Ø± {wait_time} Ø«Ø§Ù†ÛŒÙ‡ Ø¨Ø±Ø§ÛŒ Ù„ÙˆØ¯ Ø¬Ø¯ÙˆÙ„...")
                time.sleep(wait_time)
                
                # Screenshot Ø¨Ø±Ø§ÛŒ Ø¯ÛŒØ¨Ø§Ú¯
                screenshot_path = f"seleniumbase_{domain.replace('.', '_')}.png"
                sb.save_screenshot(screenshot_path, folder=".")
                print(f"ðŸ“¸ Screenshot Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {screenshot_path}")
                
#                 # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ú©â€ŒÙ„ÛŒÙ†Ú©â€ŒÙ‡Ø§
                print("ðŸ” Ø¯Ø± Ø­Ø§Ù„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ú©â€ŒÙ„ÛŒÙ†Ú©â€ŒÙ‡Ø§...")
                backlinks = self._extract_backlinks(sb, domain)
                
                if backlinks:
                    print(f"âœ… {len(backlinks)} Ø¨Ú©â€ŒÙ„ÛŒÙ†Ú© Ù¾ÛŒØ¯Ø§ Ø´Ø¯!")
                else:
                    print("âš ï¸ Ù‡ÛŒÚ† Ø¨Ú©â€ŒÙ„ÛŒÙ†Ú©ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯")
                    print("ðŸ’¡ Screenshot Ø±Ùˆ Ú†Ú© Ú©Ù† ØªØ§ Ø¨Ø¨ÛŒÙ†ÛŒ ØµÙØ­Ù‡ Ú†Ø·ÙˆØ± Ù„ÙˆØ¯ Ø´Ø¯Ù‡")
                
            except Exception as e:
                print(f"âŒ Ø®Ø·Ø§: {e}")
                import traceback
                traceback.print_exc()
        
        return backlinks
    
    def _extract_backlinks(self, sb, domain: str) -> List[Dict]:
#         """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ú©â€ŒÙ„ÛŒÙ†Ú©â€ŒÙ‡Ø§ Ø§Ø² Ø¬Ø¯ÙˆÙ„"""
        backlinks = []
        
        try:
#             # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø¬Ø¯ÙˆÙ„
            # Ahrefs Ø§Ø² Ú†Ù†Ø¯ÛŒÙ† selector Ù…Ù…Ú©Ù† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ù‡
            table_selectors = [
                'table tbody tr',
                'table tr',
                'div[role="table"] div[role="row"]',
                '[class*="backlink"] tr'
            ]
            
            rows = []
            for selector in table_selectors:
                try:
                    if sb.is_element_visible(selector):
                        rows = sb.find_elements(selector)
                        print(f"âœ… Ø¬Ø¯ÙˆÙ„ Ù¾ÛŒØ¯Ø§ Ø´Ø¯ Ø¨Ø§ selector: {selector}")
                        print(f"ðŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§: {len(rows)}")
                        break
                except:
                    continue
            
            if not rows:
                print("âŒ Ø¬Ø¯ÙˆÙ„ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯!")
                return []
            
#             # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¯Ø§Ø¯Ù‡ Ø§Ø² Ù‡Ø± Ø±Ø¯ÛŒÙ
            for i, row in enumerate(rows[:100]):  # Ø­Ø¯Ø§Ú©Ø«Ø± 100
                try:
#                     # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ Ø¯Ø± Ø±Ø¯ÛŒÙ
                    links = row.find_elements("tag name", "a")
                    
                    if len(links) >= 1:
                        url_from = links[0].get_attribute("href")
                        
#                         # ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø®Ù„ÛŒ Ahrefs
                        if url_from and not any(x in url_from for x in [
                            'ahrefs.com',
                            'help.ahrefs',
                            'youtube.com/c/Ahrefs',
                            'chrome.google.com/webstore',
                            'addons.mozilla.org',
                            'wordcount.com',
                            'ahrefstop.com',
                            'docs.ahrefs',
                            'tech.ahrefs'
                        ]):
#                             # Ú¯Ø±ÙØªÙ† Ù…ØªÙ† Ø±Ø¯ÛŒÙ
                            row_text = row.text
                            
                            # Anchor text (Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø¯Ø± Ø³ØªÙˆÙ† Ø¯ÙˆÙ… ÛŒØ§ Ø³ÙˆÙ…)
                            anchor = "N/A"
                            try:
                                cells = row.find_elements("tag name", "td")
                                if len(cells) >= 2:
                                    anchor = cells[1].text[:200]
                            except:
                                pass
                            
#                             # Ú†Ú© Ú©Ø±Ø¯Ù† nofollow
                            is_nofollow = "nofollow" in row_text.lower()
                            
                            backlink = {
                                "url_from": url_from,
                                "url_to": f"https://{domain}",
                                "anchor": anchor.strip() if anchor else "N/A",
                                "domain_rating": "N/A",
                                "nofollow": is_nofollow,
                                "source": "ahrefs_seleniumbase"
                            }
                            
                            backlinks.append(backlink)
                            
                            if i < 3:  # Ù†Ù…Ø§ÛŒØ´ 3 ØªØ§ÛŒ Ø§ÙˆÙ„
                                print(f"  {i+1}. {url_from[:60]}...")
                
                except Exception as e:
#                     # Ø§Ø¯Ø§Ù…Ù‡ Ø¨Ù‡ Ø±Ø¯ÛŒÙ Ø¨Ø¹Ø¯ÛŒ
                    continue
        
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬: {e}")
        
        return backlinks


def scrape_ahrefs_seleniumbase(
    domain: str,
    headless: bool = False,
    wait_time: int = 10
) -> List[Dict]:
    """
    Helper function Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¢Ø³Ø§Ù†
    
    Args:
        domain: Ø¯Ø§Ù…Ù†Ù‡
        headless: False = Ù…Ø±ÙˆØ±Ú¯Ø± Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒØ´Ù‡
        wait_time: Ø²Ù…Ø§Ù† Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ø±Ø§ÛŒ Ù„ÙˆØ¯ (Ø«Ø§Ù†ÛŒÙ‡)
    
    Returns:
#         Ù„ÛŒØ³Øª Ø¨Ú©â€ŒÙ„ÛŒÙ†Ú©â€ŒÙ‡Ø§
    """
    scraper = AhrefsSeleniumBase(headless=headless)
    return scraper.fetch_backlinks(domain, wait_time=wait_time)


# Test
if __name__ == "__main__":
    print("="*70)
    print("ðŸ”¥ AHREFS SCRAPER - SeleniumBase UC Mode")
    print("="*70)
    print("\nâœ¨ Features:")
    print("  âœ… Undetected Chrome (UC mode)")
    print("  âœ… Cloudflare bypass Ø¨Ø¯ÙˆÙ† Ø¯Ø®Ø§Ù„Øª Ø¯Ø³ØªÛŒ")
    print("  âœ… Auto reconnect")
    print("  âœ… Screenshot Ø¨Ø±Ø§ÛŒ Ø¯ÛŒØ¨Ø§Ú¯")
    print("="*70 + "\n")
    
    domain = input("ðŸ”‘ Ø¯Ø§Ù…Ù†Ù‡ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯: ").strip() or "kiyandaria.com"
    
    headless_input = input("ðŸ–¥ï¸  Headless mode? (y/n) [n]: ").strip().lower()
    headless = headless_input == 'y'
    
    wait_input = input("â±ï¸  Ø²Ù…Ø§Ù† Ø§Ù†ØªØ¸Ø§Ø± Ø¨Ø±Ø§ÛŒ Ù„ÙˆØ¯ Ø¬Ø¯ÙˆÙ„ (Ø«Ø§Ù†ÛŒÙ‡) [10]: ").strip()
    wait_time = int(wait_input) if wait_input.isdigit() else 10
    
    print("\n" + "="*70)
    print("ðŸš€ Ø´Ø±ÙˆØ¹...")
    print("="*70 + "\n")
    
    backlinks = scrape_ahrefs_seleniumbase(
        domain=domain,
        headless=headless,
        wait_time=wait_time
    )
    
    print("\n" + "="*70)
    print(f"ðŸ“Š Ù†ØªÛŒØ¬Ù‡: {len(backlinks)} Ø¨Ú©â€ŒÙ„ÛŒÙ†Ú©")
    print("="*70)
    
    if backlinks:
        print("\nâœ… Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ú©â€ŒÙ„ÛŒÙ†Ú©â€ŒÙ‡Ø§:\n")
        for i, link in enumerate(backlinks[:5], 1):
            print(f"{i}. {link['url_from']}")
            print(f"   Anchor: {link['anchor'][:50]}")
            print(f"   Nofollow: {'Ø¨Ù„Ù‡' if link['nofollow'] else 'Ø®ÛŒØ±'}\n")
        
#         # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± JSON
        filename = f"{domain.replace('.', '_')}_backlinks_sb.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(backlinks, f, indent=2, ensure_ascii=False)
        
        print(f"ðŸ’¾ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯ Ø¯Ø±: {filename}")
    else:
        print("\nâŒ Ù‡ÛŒÚ† Ø¨Ú©â€ŒÙ„ÛŒÙ†Ú©ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø´Ø¯")
        print("\nðŸ’¡ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª:")
        print("  1. Screenshot Ø±Ùˆ Ú†Ú© Ú©Ù†")
        print("  2. wait_time Ø±Ùˆ Ø¨ÛŒØ´ØªØ± Ú©Ù† (Ù…Ø«Ù„Ø§Ù‹ 15 Ø«Ø§Ù†ÛŒÙ‡)")
        print("  3. Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø§Ù…ØªØ­Ø§Ù† Ú©Ù†")
        print("  4. Ù…Ø·Ù…Ø¦Ù† Ø´Ùˆ Ú©Ù‡ Ø¯Ø§Ù…Ù†Ù‡ Ø¯Ø±Ø³Øª ÙˆØ§Ø±Ø¯ Ø´Ø¯Ù‡")






# =============================================================================
# 
# """
# ahrefs_seleniumbase.py - Fixed version
# ÙÛŒÚ©Ø³ Ù…Ø´Ú©Ù„ Permission Denied Ùˆ Ø¯Ø§Ù†Ù„ÙˆØ¯ ØªÚ©Ø±Ø§Ø±ÛŒ chromedriver
# """
# 
# import json
# import time
# import os
# from typing import List, Dict
# 
# # âœ… FIX: chromedriver Ø±Ùˆ Ø¨Ù‡ Ù¾ÙˆØ´Ù‡ Ù¾Ø±ÙˆÚ˜Ù‡ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ù†Ù‡ Ù†Ù‡ site-packages
# # Ø§ÛŒÙ† Ø¨Ø§ÛŒØ¯ Ù‚Ø¨Ù„ Ø§Ø² import SB Ø¨Ø§Ø´Ù‡!
# DRIVER_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), "chromedriver_local")
# os.makedirs(DRIVER_PATH, exist_ok=True)
# os.environ["CHROMEDRIVER_PATH"] = DRIVER_PATH
# 
# from seleniumbase import SB
# 
# 
# class AhrefsSeleniumBase:
#     def __init__(self, headless: bool = False):
#         self.headless = headless
#         self.base_url = "https://ahrefs.com/backlink-checker"
#     
#     def fetch_backlinks(self, domain: str, wait_time: int = 10) -> List[Dict]:
#         backlinks = []
#         print(f"ðŸš€ Ø´Ø±ÙˆØ¹ scraping Ø¨Ø±Ø§ÛŒ: {domain}")
#         
#         try:
#             with SB(uc=True, test=True, headless=self.headless, locale_code="en") as sb:
#                 try:
#                     print("ðŸŒ Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø² Ú©Ø±Ø¯Ù† Ahrefs...")
#                     url = f"{self.base_url}?input={domain}&mode=subdomains"
#                     sb.driver.uc_open_with_reconnect(url, reconnect_time=4)
#                     
#                     print("â³ ØµØ¨Ø± Ø¨Ø±Ø§ÛŒ bypass Cloudflare...")
#                     time.sleep(5)
#                     
#                     print(f"ðŸ“„ Ø¹Ù†ÙˆØ§Ù† ØµÙØ­Ù‡: {sb.get_title()}")
#                     
#                     print(f"â³ ØµØ¨Ø± {wait_time} Ø«Ø§Ù†ÛŒÙ‡ Ø¨Ø±Ø§ÛŒ Ù„ÙˆØ¯ Ø¬Ø¯ÙˆÙ„...")
#                     time.sleep(wait_time)
#                     
#                     screenshot_path = f"seleniumbase_{domain.replace('.', '_')}.png"
#                     sb.save_screenshot(screenshot_path, folder=".")
#                     print(f"ðŸ“¸ Screenshot: {screenshot_path}")
#                     
#                     print("ðŸ” Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¨Ú©â€ŒÙ„ÛŒÙ†Ú©â€ŒÙ‡Ø§...")
#                     backlinks = self._extract_backlinks(sb, domain)
#                     
#                     if backlinks:
#                         print(f"âœ… {len(backlinks)} Ø¨Ú©â€ŒÙ„ÛŒÙ†Ú© Ù¾ÛŒØ¯Ø§ Ø´Ø¯!")
#                     else:
#                         print("âš ï¸ Ù‡ÛŒÚ† Ø¨Ú©â€ŒÙ„ÛŒÙ†Ú©ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯")
#                 
#                 except Exception as e:
#                     print(f"âŒ Ø®Ø·Ø§: {e}")
#                     import traceback
#                     traceback.print_exc()
#         
#         except Exception as e:
#             print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø´Ø±ÙˆØ¹ SeleniumBase: {e}")
#             if "Permission denied" in str(e) or "Errno 13" in str(e):
#                 print("ðŸ’¡ chromedriver_local Ù¾ÙˆØ´Ù‡ Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡ - Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø³Ø¹ÛŒ Ú©Ù†")
#         
#         return backlinks
#     
#     def _extract_backlinks(self, sb, domain: str) -> List[Dict]:
#         backlinks = []
#         try:
#             table_selectors = [
#                 'table tbody tr',
#                 'table tr',
#                 'div[role="table"] div[role="row"]',
#                 '[class*="backlink"] tr'
#             ]
#             
#             rows = []
#             for selector in table_selectors:
#                 try:
#                     if sb.is_element_visible(selector):
#                         rows = sb.find_elements(selector)
#                         if rows:
#                             print(f"âœ… Ø¬Ø¯ÙˆÙ„ Ù¾ÛŒØ¯Ø§ Ø´Ø¯: {selector} ({len(rows)} row)")
#                             break
#                 except:
#                     continue
#             
#             if not rows:
#                 print("âš ï¸ Ù‡ÛŒÚ† Ø¬Ø¯ÙˆÙ„ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯")
#                 return backlinks
#             
#             for row in rows[1:]:
#                 try:
#                     cells = row.find_elements("css selector", "td")
#                     if len(cells) >= 2:
#                         backlink = {
#                             "url_from": cells[0].text.strip() if len(cells) > 0 else "N/A",
#                             "url_to": cells[1].text.strip() if len(cells) > 1 else "N/A",
#                             "anchor": cells[2].text.strip() if len(cells) > 2 else "N/A",
#                             "nofollow": False,
#                             "domain_rating": "N/A",
#                             "source": "ahrefs"
#                         }
#                         backlinks.append(backlink)
#                 except:
#                     continue
#         except Exception as e:
#             print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬: {e}")
#         
#         return backlinks
# 
# 
# def scrape_ahrefs_seleniumbase(domain: str, headless: bool = False, wait_time: int = 10) -> List[Dict]:
#     scraper = AhrefsSeleniumBase(headless=headless)
#     return scraper.fetch_backlinks(domain, wait_time=wait_time)
# 
# 
# if __name__ == "__main__":
#     print("=" * 50)
#     print("ðŸ”¥ AHREFS SCRAPER - Fixed")
#     print("=" * 50)
#     domain = input("Ø¯Ø§Ù…Ù†Ù‡: ").strip() or "kiyandaria.com"
#     backlinks = scrape_ahrefs_seleniumbase(domain=domain, headless=False, wait_time=10)
#     print(f"\nðŸ“Š Ù†ØªÛŒØ¬Ù‡: {len(backlinks)} Ø¨Ú©â€ŒÙ„ÛŒÙ†Ú©")
#     for i, link in enumerate(backlinks[:5], 1):
#         print(f"{i}. {link['url_from']}")
# 
# 
# 
# 
# 
# 
# 
# 
# 
# 
# =============================================================================











